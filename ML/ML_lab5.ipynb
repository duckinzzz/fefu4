{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:55.839976Z",
     "start_time": "2025-12-02T09:30:47.500056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print('GPU ok')\n",
    "else:\n",
    "    print('GPU not ok, CPU')"
   ],
   "id": "141d64ff6218866d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ok\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:56.470021Z",
     "start_time": "2025-12-02T09:30:55.872131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dialogues.tsv\", sep=\"\\t\")"
   ],
   "id": "29cc289fed08ee39",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:56.969013Z",
     "start_time": "2025-12-02T09:30:56.960681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text_simple(text):\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    text = re.sub(r\"<span class=participant_\\d+>\", \"\", text)\n",
    "    msgs = text.split(\"</span> \")\n",
    "\n",
    "    replics = []\n",
    "\n",
    "    for message in msgs:\n",
    "        user_id = None\n",
    "        if 'пользователь 1: ' in message:\n",
    "            message = message.replace('пользователь 1: ', '')\n",
    "            user_id = 1\n",
    "        elif 'пользователь 2: ' in message:\n",
    "            message = message.replace('пользователь 2: ', '')\n",
    "            user_id = 2\n",
    "\n",
    "        if user_id is not None:\n",
    "            message = re.sub(r\"(?<=[a-zA-Zа-яА-ЯёЁ])[^a-zA-Zа-яА-ЯёЁ ](?=[a-zA-Zа-яА-ЯёЁ])\", \" \", message)\n",
    "            message = re.sub(r\"[^a-zA-Zа-яА-ЯёЁ ]+\", \" \", message)\n",
    "            message = re.sub(r\"\\s+\", \" \", message)\n",
    "            message = message.strip()\n",
    "            if message: replics.append([user_id, message])\n",
    "\n",
    "    return replics\n",
    "\n",
    "\n",
    "def merge_consecutive_replicas(replics):\n",
    "    if not replics:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    current_user, current_msg = replics[0]\n",
    "\n",
    "    for user_id, msg in replics[1:]:\n",
    "        if user_id == current_user:\n",
    "            current_msg += \" \" + msg\n",
    "        else:\n",
    "            merged.append(current_msg.strip())\n",
    "            current_user, current_msg = user_id, msg\n",
    "\n",
    "    merged.append(current_msg.strip())\n",
    "    return merged"
   ],
   "id": "50f3035260a64048",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:59.092308Z",
     "start_time": "2025-12-02T09:30:57.147257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pairs = []\n",
    "\n",
    "min_repl_len = 2\n",
    "max_repl_len = 40\n",
    "\n",
    "for d in df[\"dialogue\"]:\n",
    "    reps = clean_text_simple(d)\n",
    "    reps = merge_consecutive_replicas(reps)\n",
    "    for i in range(len(reps) - 1):\n",
    "        q = reps[i].strip()\n",
    "        a = reps[i + 1].strip()\n",
    "        if min_repl_len <= len(q.split()) <= max_repl_len and min_repl_len <= len(a.split()) <= max_repl_len:\n",
    "            pairs.append((q, a))\n"
   ],
   "id": "6fe5298a0ab7b9c2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:59.124377Z",
     "start_time": "2025-12-02T09:30:59.108793Z"
    }
   },
   "cell_type": "code",
   "source": "len(pairs)",
   "id": "5d7fd990c61089ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130884"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:30:59.996693Z",
     "start_time": "2025-12-02T09:30:59.205588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "# Собираем все реплики в один список\n",
    "all_reps = list(chain.from_iterable(pairs))\n",
    "\n",
    "# Длины реплик в словах\n",
    "lengths = [len(r.split()) for r in all_reps]\n",
    "\n",
    "print(\"Всего реплик:\", len(all_reps))\n",
    "print(\"Средняя длина:\", np.mean(lengths))\n",
    "print(\"Медиана длины:\", np.median(lengths))\n",
    "print(\"Максимальная длина:\", np.max(lengths))\n",
    "print(\"Минимальная длина:\", np.min(lengths))\n",
    "\n",
    "# Топ-20 частот длин\n",
    "hist = Counter(lengths)\n",
    "print(\"\\nТоп частот длин:\")\n",
    "for length, freq in hist.most_common(20):\n",
    "    print(length, freq)\n",
    "\n",
    "# Словарь и частоты слов\n",
    "all_tokens = list(chain.from_iterable(r.split() for r in all_reps))\n",
    "vocab = Counter(all_tokens)\n",
    "\n",
    "print(\"\\nРазмер словаря:\", len(vocab))\n",
    "print(\"Топ-20 самых частых слов:\")\n",
    "for tok, freq in vocab.most_common(20):\n",
    "    print(tok, freq)\n",
    "\n",
    "print(\"\\nПримеры редких слов (встречаются 1 раз):\")\n",
    "for tok, freq in list(vocab.items())[:20]:\n",
    "    if freq == 1:\n",
    "        print(tok)\n"
   ],
   "id": "ae91f7e106e9c6b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего реплик: 261768\n",
      "Средняя длина: 9.505810488676996\n",
      "Медиана длины: 7.0\n",
      "Максимальная длина: 40\n",
      "Минимальная длина: 2\n",
      "\n",
      "Топ частот длин:\n",
      "3 25912\n",
      "4 25760\n",
      "5 22893\n",
      "6 20376\n",
      "2 20175\n",
      "7 17539\n",
      "8 15461\n",
      "9 13869\n",
      "10 12447\n",
      "11 10818\n",
      "12 9781\n",
      "13 8335\n",
      "14 7484\n",
      "15 6539\n",
      "16 5781\n",
      "17 4913\n",
      "18 4296\n",
      "19 3817\n",
      "20 3237\n",
      "21 2864\n",
      "\n",
      "Размер словаря: 55695\n",
      "Топ-20 самых частых слов:\n",
      "я 109454\n",
      "а 87588\n",
      "в 57041\n",
      "у 54465\n",
      "и 48962\n",
      "не 42013\n",
      "люблю 41881\n",
      "ты 41844\n",
      "на 35452\n",
      "меня 34204\n",
      "как 28188\n",
      "есть 26994\n",
      "с 25803\n",
      "очень 23765\n",
      "это 23075\n",
      "тебя 22111\n",
      "что 22064\n",
      "да 21910\n",
      "но 18919\n",
      "чем 18287\n",
      "\n",
      "Примеры редких слов (встречаются 1 раз):\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:31:00.785490Z",
     "start_time": "2025-12-02T09:31:00.060575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# длины вопросов и ответов в парах\n",
    "q_lengths = [len(q.split()) for q, a in pairs]\n",
    "a_lengths = [len(a.split()) for q, a in pairs]\n",
    "\n",
    "all_lengths = q_lengths + a_lengths\n",
    "\n",
    "print(\"Всего пар:\", len(pairs))\n",
    "print(\"Средняя длина вопроса:\", np.mean(q_lengths))\n",
    "print(\"Средняя длина ответа:\", np.mean(a_lengths))\n",
    "print(\"Минимальная длина:\", np.min(all_lengths))\n",
    "print(\"Максимальная длина:\", np.max(all_lengths))\n",
    "print(\"Медиана длины:\", np.median(all_lengths))\n",
    "\n",
    "# распределение длин\n",
    "hist = Counter(all_lengths)\n",
    "print(\"\\nТоп частот длин:\")\n",
    "for length, freq in hist.most_common(20):\n",
    "    print(length, freq)\n",
    "\n",
    "short_pairs = sum(1 for q, a in pairs if len(q.split()) <= min_repl_len or len(a.split()) <= min_repl_len)\n",
    "long_pairs = sum(1 for q, a in pairs if len(q.split()) > max_repl_len or len(a.split()) > max_repl_len)\n",
    "\n",
    "print(\"\\nКоротких пар:\", short_pairs)\n",
    "print(\"Длинных пар:\", long_pairs)\n",
    "print(\"Доля коротких:\", short_pairs / len(pairs))\n",
    "print(\"Доля длинных:\", long_pairs / len(pairs))\n"
   ],
   "id": "19957ee57ade4398",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего пар: 130884\n",
      "Средняя длина вопроса: 9.342578160814156\n",
      "Средняя длина ответа: 9.669042816539838\n",
      "Минимальная длина: 2\n",
      "Максимальная длина: 40\n",
      "Медиана длины: 7.0\n",
      "\n",
      "Топ частот длин:\n",
      "3 25912\n",
      "4 25760\n",
      "5 22893\n",
      "6 20376\n",
      "2 20175\n",
      "7 17539\n",
      "8 15461\n",
      "9 13869\n",
      "10 12447\n",
      "11 10818\n",
      "12 9781\n",
      "13 8335\n",
      "14 7484\n",
      "15 6539\n",
      "16 5781\n",
      "17 4913\n",
      "18 4296\n",
      "19 3817\n",
      "20 3237\n",
      "21 2864\n",
      "\n",
      "Коротких пар: 18862\n",
      "Длинных пар: 0\n",
      "Доля коротких: 0.14411234375477522\n",
      "Доля длинных: 0.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:31:08.508720Z",
     "start_time": "2025-12-02T09:31:00.928403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Разделяем вопросы и ответы\n",
    "questions, answers = zip(*pairs)\n",
    "\n",
    "min_num_words = 8000\n",
    "\n",
    "# Создаем токенизатор для вопросов и ответов\n",
    "tokenizer = Tokenizer(num_words=min_num_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Словарь токенизатора\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(min_num_words, len(word_index)) + 1\n",
    "\n",
    "# Преобразуем текст в последовательности чисел\n",
    "questions_seq = tokenizer.texts_to_sequences(questions)\n",
    "answers_seq = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Определяем максимальную длину последовательности\n",
    "max_len = max(max(len(seq) for seq in questions_seq),\n",
    "              max(len(seq) for seq in answers_seq))\n",
    "\n",
    "# Паддинг последовательностей\n",
    "questions_pad = pad_sequences(questions_seq, maxlen=max_len, padding='post')\n",
    "answers_pad = pad_sequences(answers_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Создаем входные и целевые последовательности для обучения\n",
    "decoder_input = answers_pad[:, :-1]  # вход для декодера\n",
    "decoder_target = answers_pad[:, 1:]  # целевая последовательность\n",
    "\n",
    "print(\"Размер словаря:\", vocab_size)\n",
    "print(\"Максимальная длина последовательности:\", max_len)\n",
    "print(\"Форма входных данных:\", questions_pad.shape, decoder_input.shape, decoder_target.shape)\n"
   ],
   "id": "7723eb90fbf1c92a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 8001\n",
      "Максимальная длина последовательности: 40\n",
      "Форма входных данных: (130884, 40) (130884, 39) (130884, 39)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:07:30.898564Z",
     "start_time": "2025-12-02T10:07:29.242308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, AdditiveAttention, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 128\n",
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len,), name='encoder_inputs')\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_len - 1,), name='decoder_inputs')\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "attention = AdditiveAttention(name='attention_layer')\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_combined = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_combined)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_attention')\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "id": "745d423bacd73be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq_attention\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, 39)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, 40, 128)      1024128     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_embedding (Embedding)  (None, 39, 128)      1024128     ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 40, 128),    131584      ['encoder_embedding[0][0]']      \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, 39, 128),    131584      ['decoder_embedding[0][0]',      \n",
      "                                 (None, 128),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 128)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " attention_layer (AdditiveAtten  (None, 39, 128)     128         ['decoder_lstm[0][0]',           \n",
      " tion)                                                            'encoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 39, 256)      0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, 39, 8001)     2056257     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,367,809\n",
      "Trainable params: 4,367,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:30:26.857196Z",
     "start_time": "2025-12-02T10:07:31.034781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Обучение с ранней остановкой\n",
    "history = model.fit(\n",
    "    [questions_pad, decoder_input],\n",
    "    decoder_target,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ],
   "id": "dc34a7ee1abdb82e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "921/921 [==============================] - 74s 74ms/step - loss: 1.4908 - accuracy: 0.1289 - val_loss: 1.4061 - val_accuracy: 0.1604\n",
      "Epoch 2/20\n",
      "921/921 [==============================] - 70s 76ms/step - loss: 1.3472 - accuracy: 0.1715 - val_loss: 1.3316 - val_accuracy: 0.1757\n",
      "Epoch 3/20\n",
      "921/921 [==============================] - 72s 78ms/step - loss: 1.2853 - accuracy: 0.1867 - val_loss: 1.2839 - val_accuracy: 0.1902\n",
      "Epoch 4/20\n",
      "921/921 [==============================] - 69s 75ms/step - loss: 1.2385 - accuracy: 0.1995 - val_loss: 1.2467 - val_accuracy: 0.2002\n",
      "Epoch 5/20\n",
      "921/921 [==============================] - 70s 76ms/step - loss: 1.2008 - accuracy: 0.2097 - val_loss: 1.2192 - val_accuracy: 0.2088\n",
      "Epoch 6/20\n",
      "921/921 [==============================] - 70s 76ms/step - loss: 1.1713 - accuracy: 0.2187 - val_loss: 1.1990 - val_accuracy: 0.2150\n",
      "Epoch 7/20\n",
      "921/921 [==============================] - 70s 76ms/step - loss: 1.1467 - accuracy: 0.2260 - val_loss: 1.1829 - val_accuracy: 0.2196\n",
      "Epoch 8/20\n",
      "921/921 [==============================] - 68s 74ms/step - loss: 1.1257 - accuracy: 0.2322 - val_loss: 1.1701 - val_accuracy: 0.2243\n",
      "Epoch 9/20\n",
      "921/921 [==============================] - 67s 73ms/step - loss: 1.1075 - accuracy: 0.2382 - val_loss: 1.1605 - val_accuracy: 0.2279\n",
      "Epoch 10/20\n",
      "921/921 [==============================] - 67s 73ms/step - loss: 1.0914 - accuracy: 0.2429 - val_loss: 1.1523 - val_accuracy: 0.2305\n",
      "Epoch 11/20\n",
      "921/921 [==============================] - 67s 73ms/step - loss: 1.0769 - accuracy: 0.2468 - val_loss: 1.1460 - val_accuracy: 0.2326\n",
      "Epoch 12/20\n",
      "921/921 [==============================] - 67s 73ms/step - loss: 1.0636 - accuracy: 0.2505 - val_loss: 1.1414 - val_accuracy: 0.2347\n",
      "Epoch 13/20\n",
      "921/921 [==============================] - 66s 72ms/step - loss: 1.0512 - accuracy: 0.2540 - val_loss: 1.1369 - val_accuracy: 0.2360\n",
      "Epoch 14/20\n",
      "921/921 [==============================] - 66s 72ms/step - loss: 1.0397 - accuracy: 0.2569 - val_loss: 1.1339 - val_accuracy: 0.2373\n",
      "Epoch 15/20\n",
      "921/921 [==============================] - 66s 72ms/step - loss: 1.0288 - accuracy: 0.2598 - val_loss: 1.1322 - val_accuracy: 0.2380\n",
      "Epoch 16/20\n",
      "921/921 [==============================] - 68s 73ms/step - loss: 1.0186 - accuracy: 0.2626 - val_loss: 1.1303 - val_accuracy: 0.2398\n",
      "Epoch 17/20\n",
      "921/921 [==============================] - 70s 76ms/step - loss: 1.0089 - accuracy: 0.2655 - val_loss: 1.1288 - val_accuracy: 0.2393\n",
      "Epoch 18/20\n",
      "921/921 [==============================] - 69s 75ms/step - loss: 0.9995 - accuracy: 0.2681 - val_loss: 1.1282 - val_accuracy: 0.2402\n",
      "Epoch 19/20\n",
      "921/921 [==============================] - 69s 74ms/step - loss: 0.9906 - accuracy: 0.2703 - val_loss: 1.1276 - val_accuracy: 0.2408\n",
      "Epoch 20/20\n",
      "921/921 [==============================] - 69s 75ms/step - loss: 0.9818 - accuracy: 0.2730 - val_loss: 1.1285 - val_accuracy: 0.2399\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:30:34.933227Z",
     "start_time": "2025-12-02T10:30:34.385224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_state_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,), name='decoder_inputs_single')\n",
    "decoder_embedding_layer = model.get_layer('decoder_embedding')\n",
    "decoder_embedded = decoder_embedding_layer(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "attention_layer = model.get_layer('attention_layer')\n",
    "context_vector = attention_layer([decoder_outputs2, encoder_outputs])\n",
    "decoder_combined = Concatenate(axis=-1)([decoder_outputs2, context_vector])\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_combined)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs + [encoder_outputs],\n",
    "    [decoder_outputs2] + decoder_states2\n",
    ")\n"
   ],
   "id": "17c9cd000b59dea7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:30:36.862304Z",
     "start_time": "2025-12-02T10:30:36.846651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "def beam_search_decode(input_seq, beam_width=3, max_response_len=7):\n",
    "    encoder_outs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "    states_value = [state_h, state_c]\n",
    "    start_token = tokenizer.word_index.get('я', 1)\n",
    "\n",
    "    sequences = [(0.0, [start_token], states_value)]\n",
    "\n",
    "    for _ in range(max_response_len):\n",
    "        all_candidates = []\n",
    "        for score, seq, states in sequences:\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states + [encoder_outs])\n",
    "\n",
    "            log_probs = np.log(output_tokens[0, -1, :] + 1e-8)\n",
    "            top_indices = np.argsort(log_probs)[-beam_width:]\n",
    "\n",
    "            for i in top_indices:\n",
    "                candidate = (score + log_probs[i], seq + [i], [h, c])\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        sequences = heapq.nlargest(beam_width, all_candidates, key=lambda tup: tup[0])\n",
    "\n",
    "    best_seq = sequences[0][1]\n",
    "    decoded_sentence = []\n",
    "    for token_idx in best_seq[1:]:\n",
    "        word = tokenizer.index_word.get(token_idx, '')\n",
    "        if word == '' or word == '<OOV>':\n",
    "            continue\n",
    "        if decoded_sentence and word == decoded_sentence[-1]:\n",
    "            continue\n",
    "        decoded_sentence.append(word)\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n"
   ],
   "id": "4286dd17088733d8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:33:09.738447Z",
     "start_time": "2025-12-02T10:33:09.696395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Виджет для ввода\n",
    "user_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Напиши сообщение...',\n",
    "    description='Ты:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Область для вывода диалога\n",
    "chat_output = widgets.Output(layout=widgets.Layout(width='80%', border='1px solid black', padding='5px'))\n",
    "\n",
    "# Кнопка отправки\n",
    "send_button = widgets.Button(description=\"Отправить\")\n",
    "\n",
    "\n",
    "# Функция обработки ввода\n",
    "def on_send_clicked(b):\n",
    "    msg = user_input.value.strip()\n",
    "    if not msg:\n",
    "        return\n",
    "\n",
    "    # Отображаем сообщение пользователя\n",
    "    with chat_output:\n",
    "        print(f\"Ты: {msg}\")\n",
    "\n",
    "    # Подготавливаем последовательность для модели\n",
    "    seq = pad_sequences(tokenizer.texts_to_sequences([msg]), maxlen=max_len, padding='post')\n",
    "    # Генерируем ответ\n",
    "    response = beam_search_decode(seq)\n",
    "\n",
    "    # Отображаем ответ модели\n",
    "    with chat_output:\n",
    "        print(f\"Бот: {response}\\n\")\n",
    "\n",
    "    user_input.value = ''  # очищаем поле ввода\n",
    "\n",
    "\n",
    "# Привязываем кнопку к функции\n",
    "send_button.on_click(on_send_clicked)\n",
    "\n",
    "# Отображаем виджеты\n",
    "display(chat_output, user_input, send_button)\n"
   ],
   "id": "af5665585237a8be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dd90f5156684f8d8e80cba7be1e99c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(value='', description='Ты:', layout=Layout(width='80%'), placeholder='Напиши сообщение...')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9701c38e6f5142ffba9e3e7007102aa6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Button(description='Отправить', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e20ab09f21a49d2960302c0e2d8c3cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:53:36.008710Z",
     "start_time": "2025-12-02T09:53:35.918685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "encoder_model.save(f'checkpoints/{timestamp}_20epch/encoder_model.h5')\n",
    "decoder_model.save(f'checkpoints/{timestamp}_20epch/decoder_model.h5')\n"
   ],
   "id": "2f846e31f0b2cbb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:53:36.133149Z",
     "start_time": "2025-12-02T09:53:36.119266Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d4a55591baf7886b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
